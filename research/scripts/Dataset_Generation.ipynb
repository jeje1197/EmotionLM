{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fFyM8aS4MtL",
        "outputId": "4f6bb6ed-e7ab-47c9-b76b-6002de18d11f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab environment detected.\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/gdrive\n",
            "Drive mounted. Output directory: /content/gdrive/MyDrive/MemoryGraph_Data\n",
            "Gemini API Key loaded from Colab Secrets.\n",
            "Gemini client successfully configured.\n",
            "\n",
            "--- Starting LLM Generation of 500 Nodes (5 DIVERSE STORIES) ---\n",
            "\n",
            "### BEGIN STORY A: Alex (Starting at Local ID 0, Global ID 0) ###\n",
            " -> Inserting initial narrative event (Node 0) for Alex...\n",
            " -> Generated 50/500 total nodes. (Current Story: A)\n",
            "    Average time per node (last 50): 4.52s\n",
            "    **Estimated Time Remaining: 33.9 minutes**\n",
            " -> Generated 100/500 total nodes. (Current Story: A)\n",
            "    Average time per node (last 50): 4.73s\n",
            "    **Estimated Time Remaining: 31.5 minutes**\n",
            "\n",
            "*** CHECKPOINT: Graph saved successfully at Global Node 99. ***\n",
            "\n",
            "### BEGIN STORY B: Maya (Starting at Local ID 0, Global ID 100) ###\n",
            " -> Inserting initial narrative event (Node 0) for Maya...\n",
            " -> Generated 150/500 total nodes. (Current Story: B)\n",
            "    Average time per node (last 50): 4.63s\n",
            "    **Estimated Time Remaining: 27.0 minutes**\n",
            " -> Generated 200/500 total nodes. (Current Story: B)\n",
            "    Average time per node (last 50): 4.81s\n",
            "    **Estimated Time Remaining: 24.0 minutes**\n",
            "\n",
            "*** CHECKPOINT: Graph saved successfully at Global Node 199. ***\n",
            "\n",
            "### BEGIN STORY C: Liam (Starting at Local ID 0, Global ID 200) ###\n",
            " -> Inserting initial narrative event (Node 0) for Liam...\n",
            " -> Generated 250/500 total nodes. (Current Story: C)\n",
            "    Average time per node (last 50): 4.26s\n",
            "    **Estimated Time Remaining: 17.8 minutes**\n",
            " -> Generated 300/500 total nodes. (Current Story: C)\n",
            "    Average time per node (last 50): 4.29s\n",
            "    **Estimated Time Remaining: 14.3 minutes**\n",
            "\n",
            "*** CHECKPOINT: Graph saved successfully at Global Node 299. ***\n",
            "\n",
            "### BEGIN STORY D: Sarah (Starting at Local ID 0, Global ID 300) ###\n",
            " -> Inserting initial narrative event (Node 0) for Sarah...\n",
            " -> Generated 350/500 total nodes. (Current Story: D)\n",
            "    Average time per node (last 50): 5.30s\n",
            "    **Estimated Time Remaining: 13.3 minutes**\n",
            " -> Generated 400/500 total nodes. (Current Story: D)\n",
            "    Average time per node (last 50): 4.75s\n",
            "    **Estimated Time Remaining: 7.9 minutes**\n",
            "\n",
            "*** CHECKPOINT: Graph saved successfully at Global Node 399. ***\n",
            "\n",
            "### BEGIN STORY E: Ben (Starting at Local ID 0, Global ID 400) ###\n",
            " -> Inserting initial narrative event (Node 0) for Ben...\n",
            " -> Generated 450/500 total nodes. (Current Story: E)\n",
            "    Average time per node (last 50): 5.96s\n",
            "    **Estimated Time Remaining: 5.0 minutes**\n",
            " -> Generated 500/500 total nodes. (Current Story: E)\n",
            "    Average time per node (last 50): 6.29s\n",
            "    **Estimated Time Remaining: 0.0 minutes**\n",
            "\n",
            "*** CHECKPOINT: Graph saved successfully at Global Node 499. ***\n",
            "\n",
            "LLM Generation Complete. Total Nodes Created: 500/500\n",
            "Time for LLM Generation and Linking: 2944.13 seconds\n",
            "\n",
            "✅ RAW Graph data (NO Embeddings) saved to: /content/gdrive/MyDrive/MemoryGraph_Data/01_DIVERSE_memory_graph_5x100_FINAL.json\n",
            "✅ Query pool saved to: /content/gdrive/MyDrive/MemoryGraph_Data/02_DIVERSE_training_queries_5x100_FINAL.json\n",
            "\n",
            "✅ FULL PIPELINE COMPLETE: DIVERSE 5x100 Data Saved.\n",
            "Total time for Pipeline: 2944.18 seconds\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# FINAL SCRIPT: 5 DIVERSE STORIES, 100 NODES EACH (N=500 TOTAL)\n",
        "# Goal: Generate 5 independent narratives with high semantic and emotional diversity.\n",
        "# Constraint: Each story is treated as a separate dataset (enforced by 'story_id').\n",
        "# ====================================================================\n",
        "\n",
        "# --- 1. IMPORTS & SETUP ---\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import networkx as nx\n",
        "from typing import List, Tuple, Union, Dict, Any\n",
        "from datetime import datetime, timezone\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    print(\"Colab environment detected.\")\n",
        "except ImportError:\n",
        "    def userdata(): return None\n",
        "    def drive(): return None\n",
        "    print(\"Running in non-Colab environment. Checkpoint saving may be affected.\")\n",
        "\n",
        "\n",
        "# --- 2. CONFIGURATION PARAMETERS ---\n",
        "NUM_STORIES = 5\n",
        "NODES_PER_STORY = 100\n",
        "TOTAL_NODES = NUM_STORIES * NODES_PER_STORY # 500 total nodes\n",
        "MODEL_NAME = \"gemini-2.5-flash\"\n",
        "BRANCHING_FACTOR = 4\n",
        "CONTEXT_BUFFER_SIZE = 5\n",
        "TIMING_INTERVAL = 50 # Report every 50 nodes\n",
        "CHECKPOINT_INTERVAL = 100 # Save checkpoint every 100 nodes\n",
        "RECENT_LINK_WINDOW_MULTIPLIER = 2\n",
        "RECENT_LINK_CANDIDATES = CONTEXT_BUFFER_SIZE * RECENT_LINK_WINDOW_MULTIPLIER\n",
        "\n",
        "# --- FILENAMES & DIRECTORY ---\n",
        "DRIVE_OUTPUT_DIR = \"/content/gdrive/MyDrive/MemoryGraph_Data\"\n",
        "RAW_GRAPH_FILENAME_BASE = \"01_DIVERSE_memory_graph_5x100_FINAL.json\"\n",
        "QUERY_FILENAME = \"02_DIVERSE_training_queries_5x100_FINAL.json\"\n",
        "# -------------------------\n",
        "\n",
        "\n",
        "# --- 3. STORY & PERSONA DEFINITIONS ---\n",
        "STORY_PROMPTS = [\n",
        "    {\n",
        "        \"story_id\": \"A\",\n",
        "        \"persona_name\": \"Alex\",\n",
        "        \"core_theme\": \"Navigating a career promotion that requires relocating away from a best friend.\",\n",
        "        \"start_time\": \"2025-01-10T15:00:00Z\",\n",
        "        \"seed_event\": \"I just received a promotion offer, but it requires relocating away from my best friend. I feel a mix of elation about the career step and deep sadness about the personal loss.\",\n",
        "        \"seed_tags\": [\"career achievement\", \"personal sacrifice\", \"major decision\", \"relocation\"],\n",
        "        \"seed_emotion\": \"Conflict (Joy & Sadness)\"\n",
        "    },\n",
        "    {\n",
        "        \"story_id\": \"B\",\n",
        "        \"persona_name\": \"Maya\",\n",
        "        \"core_theme\": \"Starting a new, highly-competitive PhD program while struggling with imposter syndrome and a demanding research supervisor.\",\n",
        "        \"start_time\": \"2024-09-01T09:30:00Z\",\n",
        "        \"seed_event\": \"The official welcome to the PhD program felt overwhelming. I'm excited by the research, but I constantly doubt if I'm smart enough to be here. My supervisor's first email was very curt.\",\n",
        "        \"seed_tags\": [\"academic pressure\", \"imposter syndrome\", \"new environment\", \"self-doubt\"],\n",
        "        \"seed_emotion\": \"Anxiety\"\n",
        "    },\n",
        "    {\n",
        "        \"story_id\": \"C\",\n",
        "        \"persona_name\": \"Liam\",\n",
        "        \"core_theme\": \"Maintaining a long-distance relationship with a partner in a different time zone and planning a major move to close the distance.\",\n",
        "        \"start_time\": \"2026-03-20T11:00:00Z\",\n",
        "        \"seed_event\": \"Had a late-night video call with my partner, celebrating a minor visa approval. It reminds me how hard the distance is, but how worth it the future planning feels. We're setting a date for me to visit.\",\n",
        "        \"seed_tags\": [\"long-distance romance\", \"future planning\", \"visa milestone\", \"communication\"],\n",
        "        \"seed_emotion\": \"Hopeful\"\n",
        "    },\n",
        "    {\n",
        "        \"story_id\": \"D\",\n",
        "        \"persona_name\": \"Sarah\",\n",
        "        \"core_theme\": \"Caring for an aging parent while balancing a full-time, emotionally draining job in nursing, leading to burnout and strain on her romantic relationship.\",\n",
        "        \"start_time\": \"2025-05-15T18:00:00Z\",\n",
        "        \"seed_event\": \"I had to rush my parent to the ER again, which made me late for work. My partner was frustrated that our dinner plans were ruined. I feel like I'm failing everyone.\",\n",
        "        \"seed_tags\": [\"caregiving\", \"burnout\", \"relationship strain\", \"guilt\"],\n",
        "        \"seed_emotion\": \"Exhaustion\"\n",
        "    },\n",
        "    {\n",
        "        \"story_id\": \"E\",\n",
        "        \"persona_name\": \"Ben\",\n",
        "        \"core_theme\": \"Training for a marathon while managing a major home renovation that keeps hitting unexpected structural problems and delaying their move-in date.\",\n",
        "        \"start_time\": \"2024-11-25T07:00:00Z\",\n",
        "        \"seed_event\": \"Completed my longest run yet—20 miles! Feeling strong physically, but the contractor just sent a photo of a termite problem in the attic, setting the renovation back by a month.\",\n",
        "        \"seed_tags\": [\"fitness goal\", \"home renovation\", \"setback\", \"physical challenge\"],\n",
        "        \"seed_emotion\": \"Frustration (physical peak, logistical low)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "# --- 4. SCHEMA DEFINITION ---\n",
        "class MemoryNode(BaseModel):\n",
        "    \"\"\"Schema for a single event/memory node in the graph, now with story_id.\"\"\"\n",
        "    event_id: int = Field(description=\"The unique, sequential integer ID within its story (0-99).\")\n",
        "    global_id: int = Field(description=\"The unique, sequential integer ID across the entire master graph (0-499).\")\n",
        "    story_id: str = Field(description=\"The unique identifier for the story this event belongs to (A, B, C, D, or E).\")\n",
        "    timestamp: str = Field(description=\"The ISO 8601 UTC timestamp of the event, strictly later than the previous event's timestamp *in this story*.\")\n",
        "    event_text: str = Field(description=\"A short, descriptive natural language text detailing the event.\")\n",
        "    semantic_tags: List[str] = Field(description=\"A list of 2-4 semantic keywords describing the event.\")\n",
        "    emotional_state: str = Field(description=\"The dominant emotional state.\")\n",
        "    semantic_vec: List[float] = Field(description=\"Placeholder: []\")\n",
        "    emotional_vec: List[float] = Field(description=\"Placeholder: []\")\n",
        "\n",
        "    # ID alias for NetworkX compatibility\n",
        "    id: int = Field(alias='id', default=None, description=\"Duplicate of global_id for graph visualization tools.\")\n",
        "\n",
        "    def __init__(self, **data):\n",
        "        super().__init__(**data)\n",
        "        if 'id' not in data or data['id'] is None:\n",
        "            self.id = self.global_id\n",
        "\n",
        "\n",
        "# --- 5. UTILITY FUNCTIONS ---\n",
        "def adaptive_wait(base_wait: float = 0.5, max_rand: float = 1.0):\n",
        "    \"\"\"Introduces a small, random delay to prevent hitting burst rate limits.\"\"\"\n",
        "    delay = base_wait + random.random() * max_rand\n",
        "    time.sleep(delay)\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Mounts drive and loads API key securely.\"\"\"\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "    print(f\"Drive mounted. Output directory: {DRIVE_OUTPUT_DIR}\")\n",
        "    try:\n",
        "        api_key = userdata.get('GEMINI_API_KEY')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API Key not found or is empty.\")\n",
        "        print(\"Gemini API Key loaded from Colab Secrets.\")\n",
        "        return api_key\n",
        "    except Exception as e:\n",
        "        print(f\"FATAL ERROR: Failed to load GEMINI_API_KEY. Details: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_partial_graph(filename: str) -> Tuple[nx.DiGraph, int, Dict[str, int]]:\n",
        "    \"\"\"Loads existing graph data to resume generation.\"\"\"\n",
        "    graph_path = os.path.join(DRIVE_OUTPUT_DIR, filename)\n",
        "    story_completion = {s[\"story_id\"]: 0 for s in STORY_PROMPTS}\n",
        "\n",
        "    if os.path.exists(graph_path):\n",
        "        try:\n",
        "            with open(graph_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            G = nx.node_link_graph(data)\n",
        "\n",
        "            # Recalculate story_completion from loaded nodes\n",
        "            if G.nodes:\n",
        "                for node_data in G.nodes.values():\n",
        "                    story_id = node_data.get('story_id')\n",
        "                    event_id = node_data.get('event_id')\n",
        "                    if story_id and event_id is not None:\n",
        "                        # Find the highest event_id for each story\n",
        "                        story_completion[story_id] = max(story_completion.get(story_id, 0), event_id)\n",
        "\n",
        "            # Find the next global ID to use\n",
        "            global_event_id = max(G.nodes) + 1 if G.nodes else 0\n",
        "\n",
        "            print(f\"\\n--- RESUMING GENERATION (FILE: {filename}) ---\")\n",
        "            print(f\"Loaded {G.number_of_nodes()} existing nodes. Starting Global ID at {global_event_id}.\")\n",
        "            print(f\"Story Progress (highest local event_id): {story_completion}\")\n",
        "            return G, global_event_id, story_completion\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"WARNING: File corrupted. Restarting from Global ID 0.\")\n",
        "            return nx.DiGraph(), 0, story_completion\n",
        "\n",
        "    return nx.DiGraph(), 0, story_completion\n",
        "\n",
        "def serialize_data(graph: nx.DiGraph, query_pool: List[str]):\n",
        "    \"\"\"Saves the RAW graph and query pool to the mounted Drive.\"\"\"\n",
        "    graph_data = nx.node_link_data(graph)\n",
        "    graph_path = os.path.join(DRIVE_OUTPUT_DIR, RAW_GRAPH_FILENAME_BASE)\n",
        "    with open(graph_path, 'w') as f:\n",
        "        json.dump(graph_data, f, indent=4)\n",
        "\n",
        "    if query_pool:\n",
        "        print(f\"\\nRAW Graph data (NO Embeddings) saved to: {graph_path}\")\n",
        "        query_path = os.path.join(DRIVE_OUTPUT_DIR, QUERY_FILENAME)\n",
        "        with open(query_path, 'w') as f:\n",
        "            json.dump(query_pool, f, indent=4)\n",
        "        print(f\"Query pool saved to: {query_path}\")\n",
        "\n",
        "def generate_complex_queries(count: int) -> List[str]:\n",
        "    \"\"\"Placeholder for complex query generation.\"\"\"\n",
        "    return [f\"Query {i} about semantic conflicts and emotional states across multiple stories.\" for i in range(count)]\n",
        "\n",
        "\n",
        "# --- 6. LLM GENERATION FUNCTION ---\n",
        "def generate_contextual_node(\n",
        "    global_id: int,\n",
        "    local_event_id: int,\n",
        "    story_config: Dict[str, Any],\n",
        "    context_buffer: List[MemoryNode]\n",
        ") -> Tuple[Union[MemoryNode, None], float]:\n",
        "    \"\"\"Generates a new MemoryNode for a specific story.\"\"\"\n",
        "    global client\n",
        "\n",
        "    story_id = story_config[\"story_id\"]\n",
        "    persona_name = story_config[\"persona_name\"]\n",
        "    last_timestamp_str = context_buffer[-1].timestamp if context_buffer else story_config[\"start_time\"]\n",
        "\n",
        "    # Format the context history for the model\n",
        "    context_history = \"\\n\".join([f\"ID {node.event_id} ({node.timestamp}): {node.event_text} [Emotion: {node.emotional_state}]\" for node in context_buffer])\n",
        "\n",
        "    # 1. Construct the detailed system prompt\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an event generator for a complex graph-based memory system.\n",
        "    Your task is to generate the next chronological event (Local Event ID {local_event_id}) in the life of the user, '{persona_name}'.\n",
        "\n",
        "    # Persona and Narrative (Story {story_id}):\n",
        "    The core theme is: '{story_config[\"core_theme\"]}'. The events must stay focused on this core theme, evolving realistically, and spanning a wide time range over 100 events.\n",
        "\n",
        "    # Strict Output Requirements:\n",
        "    1. **Format:** Output must be a single, VALID JSON object adhering to the Pydantic schema.\n",
        "    2. **Chronology:** The 'timestamp' MUST be a valid ISO 8601 UTC string and MUST be chronologically *later* than the last event's timestamp: {last_timestamp_str}. **Advance the time realistically, ensuring temporal diversity (both minutes and months).**\n",
        "    3. **Identifiers:** 'event_id' must be {local_event_id}. 'global_id' must be {global_id}. 'story_id' must be '{story_id}'.\n",
        "    4. **Placeholders:** 'semantic_vec' and 'emotional_vec' must each be an empty list: `[]`.\n",
        "\n",
        "    # Context History (Last {len(context_buffer)} Events for {persona_name}):\n",
        "    {context_history}\n",
        "    \"\"\"\n",
        "\n",
        "    # 2. Define the user prompt for the call\n",
        "    user_prompt = f\"Given the history for {persona_name} (Story {story_id}), generate Event ID {local_event_id}. The previous timestamp was {last_timestamp_str}. Advance the story related to the main theme.\"\n",
        "\n",
        "    # 3. API Call Configuration\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=system_prompt,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=MemoryNode.model_json_schema(),\n",
        "    )\n",
        "\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            start_api_call = time.time()\n",
        "            response = client.models.generate_content(\n",
        "                model=MODEL_NAME,\n",
        "                contents=user_prompt,\n",
        "                config=config,\n",
        "            )\n",
        "            elapsed_time = time.time() - start_api_call\n",
        "\n",
        "            # 4. Validation & Chronology Check\n",
        "            raw_json = response.text\n",
        "            node_dict = json.loads(raw_json)\n",
        "            new_node = MemoryNode(**node_dict)\n",
        "\n",
        "            # Additional Chronology Check\n",
        "            last_dt = datetime.fromisoformat(last_timestamp_str.replace('Z', '+00:00'))\n",
        "            new_dt = datetime.fromisoformat(new_node.timestamp.replace('Z', '+00:00'))\n",
        "\n",
        "            if new_dt <= last_dt:\n",
        "                raise ValueError(f\"Chronology Error: New timestamp ({new_dt}) is not later than previous ({last_dt}).\")\n",
        "\n",
        "            if new_node.event_id != local_event_id or new_node.global_id != global_id or new_node.story_id != story_id:\n",
        "                raise ValueError(f\"ID/Story Mismatch Error. Expected (E:{local_event_id}, G:{global_id}, S:{story_id}) but got (E:{new_node.event_id}, G:{new_node.global_id}, S:{new_node.story_id}).\")\n",
        "\n",
        "            # Perform adaptive wait before returning\n",
        "            adaptive_wait()\n",
        "            return new_node, elapsed_time\n",
        "\n",
        "        except (ValidationError, ValueError, json.JSONDecodeError) as e:\n",
        "            # Handle validation/data errors\n",
        "            print(f\"Attempt {attempt + 1}/{max_retries} failed for Story {story_id}, ID {local_event_id}. Error: {type(e).__name__} - {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                return None, 0.0 # Final failure\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle API/Network errors with Exponential Backoff\n",
        "            sleep_time = (2 ** attempt) + random.uniform(0, 1) # Exponential backoff: 1s, 2s, 4s, 8s, etc.\n",
        "            if attempt < max_retries - 1:\n",
        "                 print(f\"API Error (likely 503/429) for Story {story_id}, ID {local_event_id}. Retrying in {sleep_time:.2f}s...\")\n",
        "                 time.sleep(sleep_time)\n",
        "            else:\n",
        "                print(f\"FINAL API FAILURE for Story {story_id}, ID {local_event_id}. Giving up after {max_retries} attempts.\")\n",
        "                return None, 0.0\n",
        "\n",
        "\n",
        "# --- 7. MAIN GRAPH GENERATION LOGIC ---\n",
        "def generate_and_build_full_graph():\n",
        "    \"\"\"Generates all nodes and links across all stories with checkpointing.\"\"\"\n",
        "\n",
        "    G, current_global_id, story_progress = load_partial_graph(RAW_GRAPH_FILENAME_BASE)\n",
        "    llm_call_times = []\n",
        "\n",
        "    print(f\"\\n--- Starting LLM Generation of {TOTAL_NODES} Nodes (5 DIVERSE STORIES) ---\")\n",
        "    start_llm_time = time.time()\n",
        "\n",
        "    # Iterate through each defined story\n",
        "    for story_config in STORY_PROMPTS:\n",
        "        story_id = story_config[\"story_id\"]\n",
        "        persona_name = story_config[\"persona_name\"]\n",
        "\n",
        "        # Determine starting point for this story (0 if fresh, or resume point)\n",
        "        start_local_id = story_progress.get(story_id, 0)\n",
        "\n",
        "        # Skip if the story is already completed\n",
        "        if start_local_id >= NODES_PER_STORY:\n",
        "            print(f\"\\n--- SKIP: Story {story_id} ({persona_name}) is already complete ({NODES_PER_STORY} nodes).\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n### BEGIN STORY {story_id}: {persona_name} (Starting at Local ID {start_local_id}, Global ID {current_global_id}) ###\")\n",
        "\n",
        "        # Context buffer will be nodes from the current story only\n",
        "        context_buffer: List[MemoryNode] = []\n",
        "\n",
        "        # 1. Insert Initial Narrative Event (Node 0) if necessary\n",
        "        if start_local_id == 0:\n",
        "            print(f\" -> Inserting initial narrative event (Node 0) for {persona_name}...\")\n",
        "            initial_event = MemoryNode(\n",
        "                event_id=0, global_id=current_global_id, story_id=story_id,\n",
        "                timestamp=story_config[\"start_time\"],\n",
        "                event_text=story_config[\"seed_event\"],\n",
        "                semantic_tags=story_config[\"seed_tags\"],\n",
        "                emotional_state=story_config[\"seed_emotion\"],\n",
        "                semantic_vec=[], emotional_vec=[],\n",
        "            )\n",
        "            G.add_node(initial_event.global_id, **initial_event.model_dump())\n",
        "            context_buffer.append(initial_event)\n",
        "            current_global_id += 1 # Increment global ID after adding Node 0\n",
        "            start_local_id = 1\n",
        "\n",
        "        else: # If resuming, rebuild context buffer from last few saved nodes\n",
        "            # Find the last N nodes from THIS story only to build the buffer\n",
        "\n",
        "            # --- CONTEXT BUFFER RECONSTRUCTION (If resuming) ---\n",
        "            current_story_nodes = [data for _, data in G.nodes(data=True) if data.get('story_id') == story_id]\n",
        "            current_story_nodes.sort(key=lambda x: x.get('global_id', 0))\n",
        "\n",
        "            # Use the last CONTEXT_BUFFER_SIZE nodes of this story\n",
        "            context_nodes = current_story_nodes[-CONTEXT_BUFFER_SIZE:]\n",
        "            context_buffer.extend([MemoryNode(**data) for data in context_nodes])\n",
        "\n",
        "            # Find the current global ID based on the highest global_id added so far\n",
        "            current_global_id = max(G.nodes) + 1 if G.nodes else 0\n",
        "\n",
        "\n",
        "        # 2. Start node generation for the current story\n",
        "        for local_event_id in range(start_local_id, NODES_PER_STORY): # Generates nodes 1 through 99\n",
        "\n",
        "            node_data, node_time = generate_contextual_node(\n",
        "                global_id=current_global_id,\n",
        "                local_event_id=local_event_id,\n",
        "                story_config=story_config,\n",
        "                context_buffer=context_buffer\n",
        "            )\n",
        "\n",
        "            if node_data:\n",
        "                llm_call_times.append(node_time)\n",
        "\n",
        "                # Add node to the master graph\n",
        "                G.add_node(node_data.global_id, **node_data.model_dump())\n",
        "\n",
        "                # Link Creation (Target links backward to previous nodes *IN THIS STORY ONLY*)\n",
        "                num_links = random.randint(1, BRANCHING_FACTOR)\n",
        "\n",
        "                # Target candidates are only the nodes that share the same story_id and have a local_event_id less than the current node\n",
        "                # Note: We must use the global_id for NetworkX edge creation\n",
        "                target_candidates = [\n",
        "                    data['global_id'] for _, data in G.nodes(data=True)\n",
        "                    if data.get('story_id') == story_id and data.get('event_id') < local_event_id\n",
        "                ]\n",
        "\n",
        "                # Focus on the most recent nodes in this story's context window (local causality)\n",
        "                if len(target_candidates) > RECENT_LINK_CANDIDATES:\n",
        "                    source_candidates = target_candidates[-RECENT_LINK_CANDIDATES:]\n",
        "                else:\n",
        "                    source_candidates = target_candidates\n",
        "\n",
        "                if source_candidates:\n",
        "                    source_nodes = random.sample(source_candidates, min(num_links, len(source_candidates)))\n",
        "                    for source_global_id in source_nodes:\n",
        "                        # Add a directed edge (source -> target) with a placeholder weight\n",
        "                        G.add_edge(source_global_id, node_data.global_id, weight=random.random())\n",
        "\n",
        "                # Update context buffer\n",
        "                context_buffer.append(node_data)\n",
        "                context_buffer = context_buffer[-CONTEXT_BUFFER_SIZE:]\n",
        "\n",
        "                # CRITICAL: Increment Global ID for the next node\n",
        "                current_global_id += 1\n",
        "\n",
        "            else:\n",
        "                print(f\"Skipping Node ID {local_event_id} due to LLM failure. Stopping generation for Story {story_id}.\")\n",
        "                break # Stop this story's generation on API/validation failure\n",
        "\n",
        "            # --- PROGRESS & CHECKPOINT REPORTING ---\n",
        "\n",
        "            if len(llm_call_times) > 0 and (current_global_id) % TIMING_INTERVAL == 0:\n",
        "                # Calculate average time using the last 50 calls\n",
        "                if len(llm_call_times) >= TIMING_INTERVAL:\n",
        "                    avg_time_per_node = sum(llm_call_times[-TIMING_INTERVAL:]) / TIMING_INTERVAL\n",
        "                else:\n",
        "                    avg_time_per_node = sum(llm_call_times) / len(llm_call_times)\n",
        "\n",
        "                nodes_remaining = TOTAL_NODES - current_global_id\n",
        "                time_remaining_minutes = (nodes_remaining * avg_time_per_node) / 60\n",
        "\n",
        "                print(f\"    Generated {G.number_of_nodes()}/{TOTAL_NODES} total nodes. (Current Story: {story_id})\")\n",
        "                print(f\"    Average time per node (last {TIMING_INTERVAL}): {avg_time_per_node:.2f}s\")\n",
        "                print(f\"    Estimated Time Remaining: {time_remaining_minutes:.1f} minutes\")\n",
        "\n",
        "            # --- CRITICAL CHECKPOINT SAVE ---\n",
        "            # Save if we hit a checkpoint, or if we have finished all nodes\n",
        "            if current_global_id > 0 and (current_global_id % CHECKPOINT_INTERVAL == 0 or current_global_id == TOTAL_NODES):\n",
        "                serialize_data(G, [])\n",
        "                print(f\"\\n    CHECKPOINT: Graph saved successfully at Global Node {current_global_id-1}.\")\n",
        "\n",
        "    end_llm_time = time.time()\n",
        "    print(f\"\\nLLM Generation Complete. Total Nodes Created: {G.number_of_nodes()}/{TOTAL_NODES}\")\n",
        "    print(f\"Time for LLM Generation and Linking: {end_llm_time - start_llm_time:.2f} seconds\")\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# 8. MAIN EXECUTION\n",
        "# ====================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    try:\n",
        "        # 1. Setup Environment\n",
        "        GEMINI_API_KEY = setup_environment()\n",
        "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "        print(\"Client successfully configured.\")\n",
        "\n",
        "        # 2. Run Full Generation Pipeline\n",
        "        start_time = time.time()\n",
        "        final_memory_graph = generate_and_build_full_graph()\n",
        "\n",
        "        if final_memory_graph.number_of_nodes() < TOTAL_NODES:\n",
        "            print(f\"Script completed but only generated {final_memory_graph.number_of_nodes()}/{TOTAL_NODES} total nodes.\")\n",
        "\n",
        "        # 3. Generate and Save Final Data\n",
        "        query_pool = generate_complex_queries(TOTAL_NODES)\n",
        "        serialize_data(final_memory_graph, query_pool)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(\"PIPELINE COMPLETE: DIVERSE 5x100 Data Saved.\")\n",
        "        print(f\"Total time for Pipeline: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nSCRIPT ERROR: {type(e).__name__} - {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5r1wi8U5xnp",
        "outputId": "a42f5cb4-3c52-4d55-eb14-8f0348f0c6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/gdrive\n",
            "Gemini API Key loaded from Colab Secrets.\n",
            "Loaded graph from 01_DIVERSE_memory_graph_5x100_FINAL.json. Total nodes: 500.\n",
            "\n",
            "--- Starting MICRO-BATCH Embedding Generation for 1000 total vectors (500 nodes) ---\n",
            "   -> Processing micro-batch 1 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 2 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 3 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 4 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 5 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 6 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 7 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 8 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 9 of 10 (Size: 100)...\n",
            "   -> Processing micro-batch 10 of 10 (Size: 100)...\n",
            "\n",
            "✅ All 500 nodes enriched across 10 batches in 8.61 seconds.\n",
            "   (Average time per node: 0.02s)\n",
            "\n",
            "✅ Graph saved successfully to: /content/gdrive/MyDrive/MemoryGraph_Data/02_EMBEDDED_memory_graph_5x100_FINAL.json\n",
            "\n",
            "--- Starting Sample Generation (Ratio Linked=0/Linked=1: 2.0) ---\n",
            " -> Generated 1217 Positive Samples (Linked=1).\n",
            " -> Target Negative Samples (Linked=0): 2434\n",
            "   -> Generated 500/2434 negative samples...\n",
            "   -> Generated 1000/2434 negative samples...\n",
            "   -> Generated 1500/2434 negative samples...\n",
            "   -> Generated 2000/2434 negative samples...\n",
            " -> Final Negative Samples Generated: 2434\n",
            "TOTAL Samples Generated: 3651\n",
            "\n",
            "✅ FINAL DATASET saved successfully to: /content/gdrive/MyDrive/MemoryGraph_Data/03_CORE_training_dataset_FINAL.csv\n",
            "\n",
            "✅ DATASET CREATION PIPELINE COMPLETE. Proceed to CORE Model Training.\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# PHASE 2: FEATURE ENGINEERING AND DATASET CREATION\n",
        "# FEATURES: Time_Closeness, Semantic_Similarity, Emotional_Alignment\n",
        "# ====================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Any, Union\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import drive, userdata\n",
        "# --------------------------\n",
        "\n",
        "# --- CONFIGURATION PARAMETERS ---\n",
        "EMBEDDING_MODEL = \"text-embedding-004\"\n",
        "VECTOR_DIMENSION = 768\n",
        "NEGATIVE_SAMPLE_RATIO = 2.0\n",
        "NEGATIVE_SAMPLE_ATTEMPTS = 5\n",
        "EMBEDDING_CHUNK_SIZE = 100\n",
        "\n",
        "# Directory and filenames\n",
        "DRIVE_OUTPUT_DIR = \"/content/gdrive/MyDrive/MemoryGraph_Data\"\n",
        "RAW_GRAPH_FILENAME_BASE = \"01_DIVERSE_memory_graph_5x100_FINAL.json\"\n",
        "EMBEDDED_GRAPH_FILENAME = \"02_EMBEDDED_memory_graph_5x100_FINAL.json\"\n",
        "FINAL_DATASET_FILENAME = \"03_CORE_training_dataset_FINAL.csv\"\n",
        "\n",
        "# --- UTILITY SETUP ---\n",
        "def setup_environment():\n",
        "    \"\"\"Mounts drive and loads API key securely.\"\"\"\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "    os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "    try:\n",
        "        api_key = userdata.get('GEMINI_API_KEY')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API Key not found or is empty.\")\n",
        "        print(\"Gemini API Key loaded from Colab Secrets.\")\n",
        "        return api_key\n",
        "    except Exception as e:\n",
        "        print(f\"FATAL ERROR: Failed to load GEMINI_API_KEY. Details: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_graph(filename: str) -> nx.DiGraph:\n",
        "    \"\"\"Loads the graph from the specified file path (FIXED KeyError).\"\"\"\n",
        "    graph_path = os.path.join(DRIVE_OUTPUT_DIR, filename)\n",
        "    if not os.path.exists(graph_path):\n",
        "        raise FileNotFoundError(f\"Input graph file not found at: {graph_path}\")\n",
        "\n",
        "    with open(graph_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    try:\n",
        "        node_count = len(data['nodes'])\n",
        "    except KeyError:\n",
        "        node_count = len(data.get('node-data', []))\n",
        "\n",
        "    print(f\"Loaded graph from {filename}. Total nodes: {node_count}.\")\n",
        "\n",
        "    return nx.node_link_graph(data)\n",
        "\n",
        "def save_graph(graph: nx.DiGraph, filename: str):\n",
        "    \"\"\"Saves the graph to the specified file path.\"\"\"\n",
        "    graph_data = nx.node_link_data(graph)\n",
        "    graph_path = os.path.join(DRIVE_OUTPUT_DIR, filename)\n",
        "    with open(graph_path, 'w') as f:\n",
        "        json.dump(graph_data, f, indent=4)\n",
        "    print(f\"\\n✅ Graph saved successfully to: {graph_path}\")\n",
        "\n",
        "def save_dataframe(df: pd.DataFrame, filename: str):\n",
        "    \"\"\"Saves the final DataFrame to the specified file path.\"\"\"\n",
        "    df_path = os.path.join(DRIVE_OUTPUT_DIR, filename)\n",
        "    df.to_csv(df_path, index=False)\n",
        "    print(f\"\\n✅ FINAL DATASET saved successfully to: {df_path}\")\n",
        "\n",
        "# --- FEATURE CALCULATION ---\n",
        "\n",
        "def calculate_temporal_closeness(timestamp_source: str, timestamp_target: str) -> float:\n",
        "    \"\"\"Calculates Time_Closeness.\"\"\"\n",
        "    dt_source = datetime.fromisoformat(timestamp_source.replace('Z', '+00:00'))\n",
        "    dt_target = datetime.fromisoformat(timestamp_target.replace('Z', '+00:00'))\n",
        "    time_diff_seconds = abs((dt_target - dt_source).total_seconds())\n",
        "    return 1.0 / (time_diff_seconds + 1.0)\n",
        "\n",
        "def cosine_similarity(vec_a: List[float], vec_b: List[float]) -> float:\n",
        "    \"\"\"Calculates cosine similarity.\"\"\"\n",
        "    a = np.array(vec_a)\n",
        "    b = np.array(vec_b)\n",
        "    dot_product = np.dot(a, b)\n",
        "    norm_a = np.linalg.norm(a)\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    if norm_a == 0 or norm_b == 0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm_a * norm_b)\n",
        "\n",
        "def calculate_feature_vector(node_a_data: Dict[str, Any], node_b_data: Dict[str, Any]) -> Dict[str, float]:\n",
        "    \"\"\"Calculates all three CORE features using descriptive names.\"\"\"\n",
        "\n",
        "    time_closeness = calculate_temporal_closeness(node_a_data['timestamp'], node_b_data['timestamp'])\n",
        "    semantic_similarity = cosine_similarity(node_a_data['semantic_vec'], node_b_data['semantic_vec'])\n",
        "    emotional_alignment = cosine_similarity(node_a_data['emotional_vec'], node_b_data['emotional_vec'])\n",
        "\n",
        "    return {\n",
        "        'Time_Closeness': time_closeness,\n",
        "        'Semantic_Similarity': semantic_similarity,\n",
        "        'Emotional_Alignment': emotional_alignment,\n",
        "    }\n",
        "\n",
        "# --- BATCHING HELPER ---\n",
        "def chunk_list(lst: List[Any], n: int):\n",
        "    \"\"\"Helper to yield successive n-sized chunks from a list.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "# ====================================================================\n",
        "# STEP 1: EMBEDDING GENERATION\n",
        "# ====================================================================\n",
        "\n",
        "def generate_embeddings(G: nx.DiGraph) -> nx.DiGraph:\n",
        "    \"\"\"\n",
        "    Generates semantic and emotional embeddings using micro-batching.\n",
        "    \"\"\"\n",
        "    global client\n",
        "\n",
        "    batch_input_texts = []\n",
        "    vector_map = []\n",
        "\n",
        "    for node_id, data in G.nodes(data=True):\n",
        "        if not data.get('semantic_vec') or len(data['semantic_vec']) != VECTOR_DIMENSION:\n",
        "\n",
        "            # 1. Semantic Embedding Prompt\n",
        "            semantic_prompt = f\"Event: {data['event_text']}. Tags: {', '.join(data['semantic_tags'])}\"\n",
        "            batch_input_texts.append(semantic_prompt)\n",
        "            vector_map.append({'node_id': node_id, 'type': 'semantic'})\n",
        "\n",
        "            # 2. Emotional Embedding Prompt\n",
        "            emotional_prompt = data['emotional_state']\n",
        "            batch_input_texts.append(emotional_prompt)\n",
        "            vector_map.append({'node_id': node_id, 'type': 'emotional'})\n",
        "\n",
        "\n",
        "    if not batch_input_texts:\n",
        "        print(\"Node enrichment skipped: All nodes already contain embeddings.\")\n",
        "        return G\n",
        "\n",
        "    total_inputs = len(batch_input_texts)\n",
        "    num_batches = int(np.ceil(total_inputs / EMBEDDING_CHUNK_SIZE))\n",
        "    print(f\"\\n--- Starting MICRO-BATCH Embedding Generation for {total_inputs} total vectors ({total_inputs/2:.0f} nodes) ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Loop through chunks of the input texts (Batching)\n",
        "    for i, chunk in enumerate(chunk_list(batch_input_texts, EMBEDDING_CHUNK_SIZE)):\n",
        "        print(f\"Processing micro-batch {i+1} of {num_batches} (Size: {len(chunk)})...\")\n",
        "        try:\n",
        "            # API call for the current chunk\n",
        "            batch_result = client.models.embed_content(\n",
        "                model=EMBEDDING_MODEL,\n",
        "                contents=chunk,\n",
        "                # Task type for similarity comparison\n",
        "                config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n",
        "            )\n",
        "\n",
        "            # Extend the master list of all embeddings\n",
        "            all_embeddings.extend([np.array(e.values) for e in batch_result.embeddings])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"FATAL MICRO-BATCH ERROR in chunk {i+1}: {e}. Stopping script.\")\n",
        "            return G\n",
        "\n",
        "    # Verification check after all batches are processed\n",
        "    if len(all_embeddings) != total_inputs:\n",
        "        raise RuntimeError(f\"Total embeddings returned ({len(all_embeddings)}) does not match expected ({total_inputs}).\")\n",
        "\n",
        "    # Map the results back to the graph nodes\n",
        "    for i, vector in enumerate(all_embeddings):\n",
        "        map_entry = vector_map[i]\n",
        "        node_id = map_entry['node_id']\n",
        "        vec_type = map_entry['type']\n",
        "\n",
        "        # Convert NumPy array back to list for JSON serialization\n",
        "        vector_list = vector.tolist()\n",
        "\n",
        "        # Update the correct field in the graph\n",
        "        if vec_type == 'semantic':\n",
        "            G.nodes[node_id]['semantic_vec'] = vector_list\n",
        "        elif vec_type == 'emotional':\n",
        "            G.nodes[node_id]['emotional_vec'] = vector_list\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n✅ All {total_inputs/2:.0f} nodes enriched across {num_batches} batches in {elapsed:.2f} seconds.\")\n",
        "    print(f\"   (Average time per node: {elapsed / (total_inputs/2):.2f}s)\")\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# STEP 2: SAMPLE GENERATION (POSITIVE & NEGATIVE)\n",
        "# ====================================================================\n",
        "\n",
        "def create_training_dataset(G: nx.DiGraph, ratio: float) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates the final training dataset (Linked=1 and Linked=0 samples).\n",
        "    \"\"\"\n",
        "\n",
        "    all_samples = []\n",
        "\n",
        "    story_nodes: Dict[str, List[int]] = {}\n",
        "    for node_id, data in G.nodes(data=True):\n",
        "        story_id = data.get('story_id')\n",
        "        if story_id:\n",
        "            if story_id not in story_nodes:\n",
        "                story_nodes[story_id] = []\n",
        "            story_nodes[story_id].append(node_id)\n",
        "\n",
        "    print(f\"\\n--- Starting Sample Generation (Ratio Linked=0/Linked=1: {ratio:.1f}) ---\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # POSITIVE SAMPLES (Linked=1)\n",
        "    # ----------------------------------------\n",
        "    positive_count = 0\n",
        "\n",
        "    for source_id, target_id in G.edges():\n",
        "        source_data = G.nodes[source_id]\n",
        "        target_data = G.nodes[target_id]\n",
        "\n",
        "        if source_data['story_id'] != target_data['story_id']:\n",
        "            continue\n",
        "\n",
        "        features = calculate_feature_vector(source_data, target_data)\n",
        "        # RENAMED TARGET VARIABLE: Y -> Linked\n",
        "        features.update({'Linked': 1, 'story_id': source_data['story_id'], 'Source_ID': source_id, 'Target_ID': target_id})\n",
        "        all_samples.append(features)\n",
        "        positive_count += 1\n",
        "\n",
        "    print(f\" Generated {positive_count} Positive Samples (Linked=1).\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # PART 2: NEGATIVE SAMPLES (Linked=0)\n",
        "    # ----------------------------------------\n",
        "\n",
        "    target_negative_count = int(positive_count * ratio)\n",
        "    negative_count = 0\n",
        "\n",
        "    print(f\" Target Negative Samples (Linked=0): {target_negative_count}\")\n",
        "\n",
        "    while negative_count < target_negative_count:\n",
        "\n",
        "        story_id = random.choice(list(story_nodes.keys()))\n",
        "        current_story_nodes = story_nodes[story_id]\n",
        "\n",
        "        if len(current_story_nodes) < 2: continue\n",
        "\n",
        "        for _ in range(NEGATIVE_SAMPLE_ATTEMPTS):\n",
        "            source_id, target_id = random.sample(current_story_nodes, 2)\n",
        "\n",
        "            if G.nodes[source_id]['global_id'] >= G.nodes[target_id]['global_id']:\n",
        "                 source_id, target_id = target_id, source_id\n",
        "\n",
        "            if not G.has_edge(source_id, target_id):\n",
        "\n",
        "                source_data = G.nodes[source_id]\n",
        "                target_data = G.nodes[target_id]\n",
        "\n",
        "                features = calculate_feature_vector(source_data, target_data)\n",
        "                features.update({'Linked': 0, 'story_id': story_id, 'Source_ID': source_id, 'Target_ID': target_id})\n",
        "                all_samples.append(features)\n",
        "                negative_count += 1\n",
        "\n",
        "                if negative_count % 500 == 0:\n",
        "                    print(f\"   -> Generated {negative_count}/{target_negative_count} negative samples...\")\n",
        "\n",
        "                break\n",
        "\n",
        "    print(f\" Final Negative Samples Generated: {negative_count}\")\n",
        "    print(f\"TOTAL Samples Generated: {positive_count + negative_count}\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # PART 3: CREATE DATAFRAME\n",
        "    # ----------------------------------------\n",
        "\n",
        "    df = pd.DataFrame(all_samples)\n",
        "\n",
        "    # Shuffle the dataset before saving\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    try:\n",
        "        # 1. Setup Environment\n",
        "        GEMINI_API_KEY = setup_environment()\n",
        "        client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "        # 2. Load Raw Graph\n",
        "        G_raw = load_graph(RAW_GRAPH_FILENAME_BASE)\n",
        "\n",
        "        # 3. STEP 1: Node Enrichment (Batch Embedding) - Uses Micro-Batching\n",
        "        G_embedded = generate_embeddings(G_raw)\n",
        "        save_graph(G_embedded, EMBEDDED_GRAPH_FILENAME)\n",
        "\n",
        "        # 4. STEP 2: Feature Calculation and Sample Generation\n",
        "        final_dataframe = create_training_dataset(G_embedded, NEGATIVE_SAMPLE_RATIO)\n",
        "\n",
        "        # 5. Save Final Dataset\n",
        "        save_dataframe(final_dataframe, FINAL_DATASET_FILENAME)\n",
        "\n",
        "        print(\"\\nDATASET CREATION PIPELINE COMPLETE. Proceed to CORE Model Training.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\SCRIPT ERROR in Feature Engineering: {type(e).__name__} - {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezyD8UQyZHp3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}